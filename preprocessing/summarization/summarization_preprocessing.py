import re
import bs4
from preprocessing.common import (
    PreProcessStep,
    PreProcessedInput,
    LLM_SUMMARIZATION_PROMPT,
    LLM_SUMMARIZATION_MAX_TOKENS,
    LLM_LINKS,
)
from transformers import AutoTokenizer

class RemoveLinks(PreProcessStep):
    """Remove links from the input"""

    http_regex = "https?:\\/\\/(?:www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b(?:[-a-zA-Z0-9()@:%_\\+.~#?&\\/=]*)"

    def preprocess(
        self, input: PreProcessedInput, tokenizer: AutoTokenizer
    ) -> PreProcessedInput:
        # replace links with dummy links
        link_dict = {}
        all_links = re.findall(RemoveLinks.http_regex, input.preprocessed_input)
        counter = 0
        for link in all_links:
            if link in link_dict:
                continue
            link_dict[link] = "https://link" + str(counter) + ".servicenow.com"
            counter += 1
        # replace links
        for link in link_dict:
            input.preprocessed_input = input.preprocessed_input.replace(
                link, link_dict[link]
            )
        input.add_metadata(LLM_LINKS, link_dict)
        return input


class RemoveHtmlTags(PreProcessStep):
    """Remove HTML tags from the input"""

    def preprocess(
        self, input: PreProcessedInput, tokenizer: AutoTokenizer
    ) -> PreProcessedInput:
        # Create a BeautifulSoup object
        soup = bs4.BeautifulSoup(input.preprocessed_input, "html.parser")

        # Iterate over the BeautifulSoup object and remove the tags
        for tag in soup.find_all("*"):
            tag.decompose()
        # Print the text content of the BeautifulSoup object
        input.preprocessed_input = soup.text
        return input


class RemovePreviousSummary(PreProcessStep):
    """Remove the previous summary from the input"""

    regex_str = "[\n\s]*?[*]+\sGenerated by NowLLM\s[*]+[\n\s]*?[\d\D]+?[\n\s]*?[\n\s]*?[*]+\sGenerated by NowLLM\s[*]+[\n\s]*?"

    def preprocess(
        self, input: PreProcessedInput, tokenizer: AutoTokenizer
    ) -> PreProcessedInput:
        input.preprocessed_input = re.sub(
            RemovePreviousSummary.regex_str, "", input.preprocessed_input
        )
        return input


class ExportPrompt(PreProcessStep):
    """Export the prompt from the input"""

    regex_str = "<\|endoftext\|><\|customer\|>([\D\d]*?)<\|endoftext\|><\|agent\|>"

    def preprocess(
        self, input: PreProcessedInput, tokenizer: AutoTokenizer
    ) -> PreProcessedInput:
        prompt = re.search(ExportPrompt.regex_str, input.preprocessed_input).group(1)
        input.add_metadata(LLM_SUMMARIZATION_PROMPT, prompt)
        return input


class TruncateInput(PreProcessStep):
    """Truncates the input Text token count to less than the max token count"""

    regex_str = "(comments:|work_notes:)"
    end_of_text_token = "<|endoftext|><|customer|>"

    def single_input_total_token_size(
        self, preprocessed_input: str, tokenizer: AutoTokenizer
    ) -> int:
        # Calculate token size of the input text
        token_counter = tokenizer(preprocessed_input, return_tensors="pt")
        return token_counter["input_ids"].shape[1]

    def truncate_input(
        self, input: PreProcessedInput, tokenizer: AutoTokenizer
    )-> PreProcessedInput:
        # Calculate total token size and return input if less than max
        input_text_token_count = self.single_input_total_token_size(
            input.preprocessed_input, tokenizer)
        input.add_prediction_metadata("input_token_count", str(input_text_token_count))
        if input_text_token_count < LLM_SUMMARIZATION_MAX_TOKENS:
            return input
        comments_start_index = []
        for match in re.finditer(TruncateInput.regex_str, input.preprocessed_input):
            comments_start_index.append(match.start())
        comments_start_index_list_length = len(comments_start_index)
        iterator = 1
        processed_input = ""
        token_calc = input_text_token_count
        while token_calc > LLM_SUMMARIZATION_MAX_TOKENS:
            if iterator == comments_start_index_list_length - 1:
                eot_token_index = input.preprocessed_input.find(TruncateInput.end_of_text_token)
                sub_string_input = input.preprocessed_input[comments_start_index[0]:eot_token_index]
                processed_input = input.preprocessed_input.replace(sub_string_input, "")
                token_calc = self.single_input_total_token_size(processed_input, tokenizer)
                input.add_prediction_metadata("token_count_final", str(token_calc))
                break
            sub_string_input = input.preprocessed_input[comments_start_index[0]:comments_start_index[iterator]]
            processed_input = input.preprocessed_input.replace(sub_string_input, "")
            token_calc = self.single_input_total_token_size(processed_input, tokenizer)
            input.add_prediction_metadata("token_count_final", str(token_calc))
            iterator += 1
        input.preprocessed_input = processed_input
        return input

    def preprocess(
        self, input: PreProcessedInput, tokenizer: AutoTokenizer
    ) -> PreProcessedInput:
        input = self.truncate_input(input, tokenizer)
        return input
