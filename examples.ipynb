{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CSM = pd.read_excel('data/CSM Resolution Notes Evaluation Sept 7th.xlsx')\n",
    "df_HRSD = pd.read_excel('data/HRSD Resolution Notes Evaluation Sept 6th.xlsx')\n",
    "df_ITSM = pd.read_excel('data/ITSM Resolution Notes Evaluation Sept 7th.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Index', 'task_id', 'state', 'Case Data',\n",
       "       'NOW LLM Generated Resolution Notes', 'Reviewer',\n",
       "       'Resolution Notes is Perfect\\n(Yes/No)',\n",
       "       'Missed Important Details\\n(1/2/3)', 'Had Made-up Details\\n(1/2/3)',\n",
       "       'Mixed up who said what\\n(1/2/3)',\n",
       "       'Had too much unnecessary info\\n(1/2/3)', 'Golden Resolution Notes',\n",
       "       'Reviewer's notes', 'Linguist Comments', 'Linguist'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for context in df_CSM['Case Data']:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Resolution notes defect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snow.internatg.anirudhsreeram/home/miniconda3/envs/infer1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, pandas as pd, numpy as np, re, json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, AutoConfig\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tqdm import tqdm\n",
    "from preprocessing.preprocessing_pipeline import PreProcessingPipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import LogitsProcessorList\n",
    "from custom_logits_processor import (NoRepeatNGramLogitsProcessor,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['task_id', 'context', 'prompt', 'response', 'golden resolution notes',\n",
      "       'source_type', 'hallucination_summac_asym_precision',\n",
      "       'hallucination_summac_sym_precision', 'completeness_summac_asym_recall',\n",
      "       'completeness_summac_sym_recall', 'entity_hallucination_score'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "model_id = '/mnt/atg_platform/models/now_llm_chat/v0.4.0-rc2'\n",
    "cache = 'cache_model'\n",
    "eval_data = pd.read_json('data/resolution_notes_0.4.0.json')\n",
    "print(eval_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    cache_dir=cache,\n",
    "    trust_remote_code=True,\n",
    "    use_cache=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model_base.to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|end|>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id = 49155\n",
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(model_base, tokenizer, eval_data):\n",
    "    outputs_response = []\n",
    "    for idx, record in tqdm(eval_data.iterrows()):\n",
    "        inputs = record[\"prompt\"]\n",
    "        custom_logits_processors = LogitsProcessorList()\n",
    "        no_repeat_ngram_size = 10\n",
    "        custom_logits_processors.append(\n",
    "            NoRepeatNGramLogitsProcessor(no_repeat_ngram_size, tokenizer)\n",
    "        )\n",
    "\n",
    "        # preprocessing\n",
    "        steps = [\"remove_previous_summary\", \"truncate_input\"]\n",
    "        pipeline = PreProcessingPipeline()\n",
    "\n",
    "        def preprocess(text):\n",
    "            # text = (\n",
    "            #     text\n",
    "            #         .replace(\"<|system|>\", \"<|system|>\\n\")\n",
    "            #         .replace(\"<|endoftext|><|customer|>\", \"<|end|>\\n<|user|>\\n\")\n",
    "            #         .replace(\"<|endoftext|><|agent|>\", \"<|end|>\\n<|assistant|>\")\n",
    "            # )\n",
    "\n",
    "            return pipeline.preprocess(text, tokenizer, steps).preprocessed_input\n",
    "\n",
    "        inputs = preprocess(inputs)\n",
    "        \n",
    "        # need to add current inputs\n",
    "        #inputs = '<|system|>\\n' + inputs + '<|end|>\\n<|user|>\\n' + prompt + '<|end|>\\n<|assistant|>'\n",
    "        \n",
    "        cuda_device = 'cuda:0'\n",
    "        inputs_tokenized = tokenizer(inputs, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs_tokenized = {k: v.to(cuda_device) for k, v in inputs_tokenized.items()}\n",
    "            outputs = model_base.generate(\n",
    "                    input_ids=inputs_tokenized[\"input_ids\"],\n",
    "                    attention_mask=inputs_tokenized[\"attention_mask\"],\n",
    "                    max_new_tokens=500,\n",
    "                    temperature=0.3,\n",
    "                    num_beams=1,\n",
    "                    use_cache=True,\n",
    "                    do_sample=True,\n",
    "                    logits_processor=custom_logits_processors,\n",
    "                    num_return_sequences=1,\n",
    "                    repetition_penalty=1.05,\n",
    "                    eos_token_id=tokenizer.eos_token_id,\n",
    "\n",
    "            )\n",
    "\n",
    "            outputs = outputs[:, inputs_tokenized[\"input_ids\"].shape[1] :]\n",
    "\n",
    "            single_result = tokenizer.batch_decode(\n",
    "                outputs.detach().cpu().numpy(), skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            outputs_response.append(single_result[0])\n",
    "    # add outputs to eval_data\n",
    "    eval_data[\"response_with_eos_49155\"] = outputs_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "175it [05:39,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "generate_summary( model_base, tokenizer, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data.to_excel('OUTPUTS/resolution_notes_0.4.0_with_eos_49155.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('OUTPUTS/resolution_notes_0.4.0_with_eos_49155.json','w') as f:\n",
    "    json.dump(eval_data.to_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('OUTPUTS/10_13_2023/inputs_P1_outputs.json','r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for item in data:\n",
    "    item[\"Hallucination\"] = 1\n",
    "\n",
    "with open('OUTPUTS/10_13_2023/inputs_P1_outputs_hal.json','w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "14\n",
      "116\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('OUTPUTS/10_13_2023/inputs_P1_outputs_hal.json','r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "ones = []\n",
    "zeros = []\n",
    "for item in data:\n",
    "    if item[\"Hallucination\"] == 1:\n",
    "        ones.append(item)\n",
    "    else:\n",
    "        zeros.append(item)\n",
    "\n",
    "print(len(ones))\n",
    "print(len(zeros))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open('OUTPUTS/10_16_2023/inputs_P1_outputs_RS2.json','r') as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame(data)\n",
    "# add columns\n",
    "df[\"COMPLETENESS Missed important details (1/2/3)\"] = \"\"\n",
    "df[\"HALLUCINATION Had made up details (1/2/3)\"] = \"\"\n",
    "df[\"Put details in wrong places (1/2/3)\"] = \"\"\n",
    "df[\"Had unnecessary info (1/2/3)\"] = \"\"\n",
    "df[\"Reviewer Notes (COMPLETENESS)\"] =\"\"\n",
    "df[\"Reviewer Notes (HALLUCINATION)\"] =\"\"\n",
    "\n",
    "df.to_excel('OUTPUTS/10_16_2023/inputs_P1_outputs_RS2.xlsx') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
